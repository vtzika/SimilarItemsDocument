\chapter{Analysis}

We provide a further analysis about the results of the experiments described in the previous chapter. We also present our initial assumptions about each experiment and we are presenting extra experiments conducted in order to prove them.

\section{Stemming experiment}

Preprocessing is a good approach to improve the effectiveness of retrieval systems. The document collection consists of classifieds created by regular users and contains a lot of noise which should be extracted before indexing takes place. Removing the noise can improve query efficiency. Thus, we hypothesize that using stemming will retrieve more relevant results. For example, ``cars'' will be stemmed in ``car''. If we have a document related to ``car'', it will be retrieved as well.

We make one initial experiment to measure if any change on performance occurs when we use stemming in the preprocessing phase. In this phase we use two different query models and we compare the result lists with stemming used in the preprocessing and without.

We preprocess the document collection and we convert it in lowercase, remove stopwords, replace punctuations with spaces and remove words with one or two characters. Also, for experimentation purposes we compare two approaches (with or without  stemming). Stemming is based on the snowball stemmer. Then, the data saved in SGML formatted documents.



\begin{table}[H]
\begin{center}
\caption{System performance  using precision at first five results measure (P@5) of title (T), title using LLR (T-LLR) query models with and without stemming using three retrieval strategies (BM25, LM, TfIdf) and two types of ground truth (editorial and click logs).}
\label{table:stemmSystems}
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{3}{c}{Editorial} & & \multicolumn{3}{c}{Click logs} \\
\midrule
 & BM25 & LM & TfIdf &   & BM25 & LM & TfIdf \\
\midrule
T &  \textbf{0.6560} &  0.5980 & 0.626 &   &            \textbf{0.1680} & 0.1660 & 0.1540 \\
T-stemming 	& 0.6040 & 0.5660 & 0.5640 &   &	 		0.1640 & 0.1660 & 0.1600 \\
T-LLR & \textbf{0.5660} & 0.5160 & 0.5280 &   &	 		\textbf{0.1580} & 0.1540 & 0.1500 \\
T-stemming-LLR & 0.5280 & 0.5160 & 0.4980 &   &	 		\textbf{0.1580} & 0.1560 & 0.1460 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}




Based on the results on table \ref{table:stemmSystems} using editorial evaluation, stemming decrease the precision at least 0.3 and the maximum of 0.5 at Okapi BM25. There is no difference in precision in the query model with LLR on title field using LM.
Based on the results using click logs evaluation, there is no big decrease in the results with stemming but neither an improvement. The only case that a small increase in performance happened is on the title query using TFIDF.

We believe that the reason of the previous results is that stemming negatively affects the queries accuracy. For instance, if we search for ``organization'' and our stemmer removes the reasonably common suffix ``ization'' we end up with classifieds about ``organ''. Also, we found examples that didn't retrieve any relevant classifieds on the first five results, while there were at least two relevant results in the systems without stemming. Also, we observe that the same classifieds in the result list have lower score due to the fact that the frequency of stemmed terms are greater.

For further analysis, we are presenting the results of the table \ref{table:stemmRelIrrel} with the count of relevant results retrieved only when stemming used and the count of relevant results retrieved only in the systems without stemming. We used the Okapi Title query model since we found the biggest difference in the performance.

From the ranked classified list of title query model using BM25 and the ranked classified list of the T query model using stemming. 


\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with title query model (T) and BM25 without stemming, number of examples retrieve uniquely this number of classifieds with title query model and BM25 with stemming (T-stemming).}
\label{table:stemmRelIrrel}
\begin{tabular}{lcr}
\midrule
Relevant classifieds & T & T-stemming \\
\midrule
	1 & 31 & 26 \\
	2 & 12 & 5  \\
	3 & 3 & 2 \\
	4 & 1 & 0  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


As you can see, the uniquely retrieved results from the non stemmed system are more than the uniquely retrieved by the stemmed system. There are 31 visited classifieds retrieved one unique relevant classified that is not retrieved in the first 5 results of the stemmed systems. Also, there are 16 visited classifieds retrieved more than one unique relevant classified that is not retrieved in the first five results of the stemmed systems.

In sum, none of the systems using stemming improve the performance of our baseline thus we decide not to use it in the rest of the experiments.

\section{Query modeling}

We hypothesize that if we add information in our baseline, we will improve the precision of the five first results. In the results provided in the results chapter, this assumption is proved. Adding the description in the baseline's query model improved the performance. Furthermore, adding the attributes and category had the best precision in all three retrieval strategies using either editorial evaluation or click log evaluation.

Also, in the moment we made the editorial evaluation we observed the following:

\begin{list}{.}{}
\item{In some classifieds like men/women shoes, there is no difference in the text of the classifieds except of the category name. Thus, queries like title and description do not perform as well as query models including the category name.}
\item{Also in the same case, attributes will add extra value due to the extra information about the size and the kind of shoe.}
\item{In the case of products sale classifieds as original classified, we have as result classifieds with parts for this product. However, this is not always relevant.}
\end{list}
To further analyze the results we are presenting the table \ref{table:titleVsTD} with the extra relevant results retrieved when description is added on the query model.


\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with title and description query model and BM25 (T+D), number of examples retrieve uniquely this number of classifieds with title query model and BM25 (T).}
\label{table:titleVsTD}
\begin{tabular}{lcr}
\midrule
Relevant classifieds & T+D & T \\
\midrule
	1 & 15 & 26 \\
	2 & 12 & 11  \\
	3 & 13 & 3 \\
	4 & 4 & 1  \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

As you can see, the query model with title and description has a lot of relevant results retrieved while they are not retrieved from title query model. Although there are much more cases that only one relevant result retrieved uniquely from only title query (26 versus 15), the number of the rest of the cases (that more than one uniquely retrieved from the description and title query model) outperform it e.g 13 versus 3. In total 44 relevant classifieds uniquely retrieved from title and description query model and 41 from title query model.



Furthermore, we are presenting the table \ref{table:tVsEntRank} with the uniquely relevant results retrieved when all the fields are in the query model in a comparison with the description query model.

\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with all fields in query model and BM25 (T+D+A+C), number of examples retrieve uniquely this number of classifieds with title and description query model and BM25 (T+D).}
\label{table:tVsEntRank}
\begin{tabular}{lcr}
\midrule
Relevant classifieds &  T+D+A+C & T+D \\
\midrule
	1 & 39 & 28 \\
	2 & 18 & 18 \\
	3 & 8 & 5 \\
	4 & 6 & 3 \\
	5 & 1 & 1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

As you can see from the table \ref{table:tVsEntRank}, 72 uniquely retrieved results from all the fields query model and 55 from title and description query model. Furthermore, the uniquely retrieved results from all the fields are more or equal than description query model. Moreover, in table \ref{table:tVsEnt}, we can see that 219 classifieds are retrieved by both query models. Title query model retrieved 109 relevant results are not retrieved on the entire query model while entire retrieved 146, That's a good indication that, the extra information on the attributes and category improves the results due to the fact that more relevant classifieds retrieved.

\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved by title (T), all fields (T+D+A+C) query model and number of common retrieved relevant classifieds.}
\label{table:tVsEnt}
\begin{tabular}{lcr}
\midrule
 Query model &  Total number \\
\midrule
	T & 328 \\
	T+D+A+C & 365 \\
	Common & 219 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

Also, we are presenting the table \ref{table:allVST} with the uniquely relevant results retrieved when all the fields are in the query model in a comparison with the baseline.


\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with all fields in query model and BM25 (T+D+A+C), number of examples retrieve uniquely this number of classifieds with title query model and BM25 (T).}
\label{table:allVST}

\begin{tabular}{lcr}
\midrule
Relevant classifieds &  T+D+A+C & T \\
\midrule
	1 & 37 & 22 \\
	2 & 16 & 19 \\
	3 & 12 & 8 \\
	4 & 9 & 5  \\
	5 & 1 & 1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}




The uniquely retrieved results from all the fields are more or equal than our baseline except of the case that 2 unique results are retrieved in the top five results. In total, 75 uniquely retrieved results from all the fields query model and 55 from title query model. Furthermore, in table \ref{table:qmEd}, we present more measures for all the query models that proves the same assumptions. Precision at first ten results is increased when title is added to the baseline and has the best score when attributes and category added as well. Also, similar observations we had using MAP or Rprec as measurements. However, in \ref{table:qmCl}, the results are not so clear but they they don't disprove our initial assumptions.

We also observed that query model with attributes and category indicates the lowest precision in a comparison with the rest of the query models. We hypothesize that not enough information exists to attributes and category, thus the results will be negative. To further prove that assumption, we are presenting the table \ref{table:AcVsT} with the uniquely relevant results retrieved when attributes and category are only in the query model in a comparison with the baseline.


\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with attributes and category in query model and BM25 (A+C), number of examples retrieve uniquely this number of classifieds with title query model and BM25 (T).}
\label{table:AcVsT}
\begin{tabular}{lcr}
\midrule
Relevant classifieds &  A+C & T \\
\midrule
	1 & 16 & 17 \\
	2 & 12  & 16 \\
	3 & 16 & 18 \\
	4 & 16 & 29 \\
	5 & 5 & 9 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



It is obvious in the previous results that query model with only attribute and category has less uniquely retrieved results than our baseline. Also, in the results chapter the query model using only attributes and category has the lowest precision, thus we can conclude that attributes and category alone are not containing enough information to improve the precision of query with title.


 \textbf{Query models with discriminative terms}
Our assumption was that discriminative terms will retrieve less results but more accurate due to the fact that a more accurate query is created. Thus, the precision at first five will be increased. To prove this assumption, we present the results of the table \ref{table:discP5} with the six different systems.


\begin{table}[H]
\begin{center}
\caption{System performance   using precision at first five results measure (P@5) of title (T), title using LLR (T-LLR), title using stemming (T-stemming), title using both LLR and stemming (T-LLR stemming), all fields (T+D+A+C) and all fields using LLR (T+D+A+C-LLR) query models and two types of ground truth (editorial and click logs).}
\label{table:discP5}

\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{3}{c}{Editorial} & & \multicolumn{3}{c}{Click logs} \\
\midrule
 & BM25 & LM & TfIdf &   & BM25 & LM & TfIdf \\
\midrule
T & \textbf{0.6560} &  0.5980 & 0.626 &   &      \textbf{0.1680} & 0.1660 & 0.1540 \\
T-LLR & 0.5660 & 0.5160 & 0.5280 &   &	 		 0.1580 & 0.1540 & 0.1500 \\
\midrule
T-stemming 	& \textbf{0.6040} & 0.5660 & 0.5640 &   &	 0.1640 & \textbf{0.1660} & 0.1600 \\
T-stemming-LLR & 0.5280 & 0.5160 & 0.4980 &   &	 0.1580 & 0.1560 & 0.1460 \\
\midrule
T+D+A+C & \textbf{0.7300} & 0.6460 & 0.6860 &   &		 \textbf{0.1720} & 0.1700 & 0.1580 \\
T+D+A+C-LLR & 0.6360 & 0.5400 & 0.5740 &   &	 0.1600 & 0.1640 & 0.1540 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


Based on the results, all the systems are negatively affected in a comparison with the systems with the discriminative terms. Using the editorial evaluation, big decrease from all the fields query model to LLR all the fields query model in all three retrieval strategies is indicated. Smaller decreases in the rest of the systems are shown but none of them show any improvement. Since using stemming and relevance in the title query hurts the performance even more than 0.10, we decided not to use it in the rest of the query models.


We hypothesize that the reason of the previous results is that LLR is removing some important information thus relevant results are not retrieved. To prove that, we are presenting the table \ref{table:LLRP5} with the count of relevant results retrieved only when LLR is used in a system and the count of relevant results retrieved only in the same system without the use of LLR.

\begin{table}[H]
\begin{center}
\caption{Number of relevant classifieds retrieved on the top five results, number of examples retrieve uniquely this number of classifieds with LLR and all fields query model (T+D+A+C-LLR) in the query model and BM25, number of examples retrieve uniquely this number of classifieds with all fields query model (T+D+A+C) and BM25.}
\label{table:LLRP5}

\begin{tabular}{lcr}
\midrule
Relevant classifieds &  T+D+A+C-LLR & T+D+A+C \\
\midrule
	1 & 39 & 38 \\
	2 & 10 & 12  \\
	3 & 2 & 1 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



No obvious difference is observed in table \ref{table:LLRP5} thus this subject will be added on the future work due to time constraint. One possible explanation that precision is harmed is that the the same relevant classifieds are scored less when LLR is used due to less word matching. Thus, they retrieve the same relevant classifieds but with different rank.


 \textbf{Query models with pseudo relevance feedback}

 Pseudo relevance feedback extends the query and more relevant classifieds retrieved. To prove this assumption, we analyze the results of the table \ref{table:PseudoP5} with two different systems.

\begin{table}[H]
\begin{center}
\caption{System performance   using precision at first five results measure (P@5) of all fields (T+D+A+C) and all fields using pseudo relevance feedback (T+D+A+C-Pseudo) query models and two types of ground truth (editorial and click logs).}
\label{table:PseudoP5}
\begin{tabular}{lccccccc}
\toprule
 & \multicolumn{3}{c}{Editorial} & & \multicolumn{3}{c}{Click logs} \\
\midrule
& BM25 & LM & TfIdf &   & BM25 & LM & TfIdf \\
\midrule
	T+D+A+C & \textbf{0.7300} & 0.6460 & 0.6860 &   &		 \textbf{0.1720} & 0.1700 & 0.1580 \\
	Pseudo(T+D+A+C) & \textbf{0.7300} & 0.4920 & 0.6520 &   & \textbf{0.1720} & 0.1620 & 0.1500 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The results of the table \ref{table:PseudoP5} show that in the case of LM and TfIdf there is a decrease in the P@5, but in the case of Okapi it remains stable. Since the pseudo relevance feedback is an expensive procedure due to the fact that it needs to retrieve results, take feedback and then to retrieve new result list, we decide that we will not use pseudo relevance feedback on the rest of the query models.



\section{Retrieval methods}

In the results provided in results chapter, we can see that Okapi BM25 performs better than the other two. In the following table we further prove the same assumption. We present the count of relevant and irrelevant results retrieved of each retrieval strategy.

\begin{table}[H]
\begin{center}
\caption{Retrieval strategies (BM25, LM, TfIdf), count of relevant results and count of irrelevant results retrieved from each one.}

\begin{tabular}{lcr}
\midrule
 & Relevant & Irrelevant  \\
\midrule
	BM25 & \textbf{3121} & 1879 \\
	LM & 2399 & 2101 \\
	TfIdf & 2633 & \textbf{1858} \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

It is obvious that Okapi BM25 retrieves the biggest number of relevant results. Though, TfIdf retrieves 21 less irrelevant results. However, the fact that the precision of Okapi BM25 is always the greatest in all query models, makes us enough confident to say that Okapi BM25 is better than the other two.

\section{Late data fusion}

In the table \ref{table:ldfP5}, we can see that late data fusion methods are not performing better than the best individual system. We were expecting that combMIN will have the worst precision since it takes into account the smallest score. But, we believed that combMNZ will perform better than the best individual system due to the fact that it boosts score and rank of a relevant classified that is agreed upon many systems.

From the other side, using the click logs evaluation the precision is improved on the weighted fusion methods (WcombSUM, WcombMNZ, WcombWW), but the difference is just 0.02 which is not enough to make us confident to say that they perform better.

Our assumption is that we have these results because we fused all the systems together. Some of the systems are not performing as well as others. To prove that assumption, we present the table \ref{table:relIrelLDF} with the total number of relevant and irrelevant classifieds retrieved by a specific number of systems.


\begin{table}[H]
\begin{center}
\caption{Number of systems, total number of relevant and irrelevant classifieds retrieved by the specific number of systems.}
\label{table:relIrelLDF}
\begin{tabular}{lcr}
\midrule
 Systems & Irrelevant & Relevant \\
 \midrule
1 & 10229 & 135 \\
2 & 4034 & 115 \\
3 & 1614 & 75 \\
4 & 1168 & 49 \\
5 & 704 & 57 \\
6 & 549 & 71 \\
7 & 419 & 61 \\
8 & 312 & 74 \\
9 & 266 & 47 \\
10 & 227 & 53 \\
11 & 154 & 37 \\
12 & 143 & 46 \\
13 & 109 & 59 \\
14 & 97 & 41 \\
15 & 79 & 46 \\
16 & 65 & 36 \\
17 & 45 & 21 \\
18 & 47 & 31 \\
19 & 38 & 38 \\
20 & 31 & 34 \\
21 & 32 & 41 \\
22 & 38 & 34 \\
23 & 19 & 42 \\
24 & 19 & 33 \\
25 & 4 & 32 \\
26 & 1 & 20 \\
27 & 1 & 25 \\
28 & 1 & 31 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The results of the table \ref{table:relIrelLDF} indicate that there are only three irrelevant classifieds that are retrieved from more than 25 systems while 31 relevant results retrieved by all all the systems. On the other hand, less than five systems retrieved more than a thousand irrelevant classifieds and around 100 relevant classifieds.  Since the number of relevant results retrieved by many systems is not big, fusion systems such as combMNZ are not improving the precision of the individual systems. The different information in the query models is a possible reason of these results but it will be part of future investigation.


For future work further analysis, more measures results are presented on \ref{table:ldf} using three retrieval models and two ground truth types (editorial and click logs).

To further analyze these results, we conduct one extra experiment with combANZ keeping stable all systemic differences like stemming, query modeling but retrieval strategy. We compare the results with combANZ result from the fusion of all individual systems to see if any improvement occurs.



\begin{table}[H]
\begin{center}
\scriptsize
\caption{Systems used on the combANZ with stable systemic differences are presented. Explanation is given for the use of fields use in the query model, log likelihood ratio (LLR), stemming and pseudo relevance feedback (PRF). }
\label{table:combANZSystems}

\begin{tabular}{lccccr}
\midrule
Name & Fields & Stemming & LLR & PRF \\
\midrule
T & title & no & no & no \\
T-LLR & title & no & yes & no \\
T stemming & title & yes & no & no \\
T-LLR stemming & title & yes & yes & no \\
T+D  & title, description & no & no & no \\
T+D+A+C & title, description, category, attributes & no & no & no \\
A+C & attributes, category & no & no & no \\
T+D+A+C-LLR & title, description, category, attributes & no & yes & no \\
T+D+A+C-Pseudo & title, description, category, attributes & no & no & yes \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}[H]
\begin{center}
\footnotesize
\caption{System performance   using precision at first five results measure (P@5) of combANZ keeping stable systemic differences in a comparison with combANZ merged from all systems using editorial and click logs evaluation. Query models used are title (T), title using LLR (T-LLR), title using stemming (T-stemming), title and description (T+D), all the fields (T+D+A+C) and attribute and categories (A+C), all the fields using LLR (T+D+A+C-LLR), all the fields using pseudo relevance feedback (T+D+A+C-pseudo) query models and two types of ground truth (editorial and click logs).}
\label{table:combANZP5}
\begin{tabular}{lcr}
\midrule
 Query model & Editorial & Click logs \\
 \midrule
	T & 0,2740 & 0.0700 \\
	T-LLR & 0,2600 & 0.0520 \\
	T stemming & 0,1520 & 0.0180 \\
	T-LLR stemming & 0,0800 & 0.0060 \\
	T+D & 0,2660 & 0.0220 \\
	T+D+A+C & 0,2120 & 0.0200 \\
	A+C & 0,1600 & 0.0080 \\
	T+D+A+C-LLR & 0,1780 & 0.0280 \\
	T+D+A+C-Pseudo & 0,1440 & 0.0280 \\
	\midrule
	combANZ & 0.0800 & 0.0020 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}



As you can see in the table \ref{table:combANZP5}, the combANZ is improved when the systemic differences kept stable. However they didn't outperform the precision of the individuals (see table \ref{table:qmP5}).


Furthermore, more measures results are presented in table \ref{table:combANZ} using three retrieval models and two ground truth types (editorial and click logs).

To conclude, the fusion of the results doesn't outperform the performance of the best individual system. However, we achieved to improve the effectiveness of combANZ when systemic differences kept stable. Thus, in the future plans, we can do the same experiment for the rest of the fusion method as well.

\section{Diversification}

As you can see in the bar graphs in the results chapter, there is no big improvement in precision using either editorial or click logs evaluation and the alternative diversification approaches. Our assumption is that our results are too diversified already. This assumption can be proved if the precision in the individual systems is the same with the diversified systems. The table \ref{table:MMRP5} presents the results of BM25 individual systems and BM25 diversified systems using editorial evaluation and click logs evaluation.



\begin{table}[H]
\footnotesize
\begin{center}
\caption{System performance   using precision at first five results measure (P@5) of diversified and individual systems using title (T), title and description (T+D), all the fields (T+D+A+C) and attribute and categories (A+C) query models, okapi BM25 retrieval strategy (BM25) and two types of ground truth (editorial and click logs).
}
\label{table:MMRP5}

\begin{tabular}{lccccc}
\toprule
 & \multicolumn{2}{c}{Editorial evaluation} &   & \multicolumn{2}{c}{Click logs evaluation} \\
\midrule
 & Individuals & Diversified &   & Individuals & Diversified \\
\midrule
T & 0.6560 & 0.6580 &   & 0.1680 & 0.1680 \\
(T+D) & 0.6660 & 0.6700 &   & 0.1700 & 0.1700 \\
(T+D+A+C) & 0.7300 & 0.7300 &   & 0.1720 & 0.1760 \\
(A+C) & 0.4800 & 0.4460 &   & 0.0920 & 0.0900 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


The differences in precision are really low in both evaluation approaches. We believe that the reason is that the initially retrieved results from the individual strategies are already diversified.

Further analysis for each of the alternative diversification methods is provided on the following paragraphs.

 \paragraph{MMRalt1}

The fact that the precision is not affected by this diversification approach is a positive sign. We take into account only the last one selected instead of all the previous. From the other side that can be a prove that our classifieds are enough diversified that are not affected by any diversification method.


 \paragraph{MMRalt2}

In this approach we used the average similarity in the calculation of the new score instead of choosing the maximum one. Thus, this approach can be used as an alternative since it doesn't affect the precision.


 \paragraph{MMRaltAvgLast4}

 Same with the MMRalt1 approach, is a good sign that the precision is not affected by this algorithm due to the fact that we are comparing only the previous four selected results.


Further measurements and results for the rest of the retrieval models can be seen on \ref{table:mmrEd} and \ref{table:mmrCl} for MMR, \ref{table:mmrAlt1Ed} and \ref{table:mmrAlt1Cl} for MMRalt1, \ref{table:mmrAlt2Ed} and \ref{table:mmrAlt2Cl} for MMRalt2,  \ref{table:mmrAltAvgLast4Ed} and \ref{table:mmrAltAvgLast4Cl} for MMRaltAvgLast4.


The analysis of our results gives us ideas for future work that are covered in the next chapter. The summary and conclusion of this work is provided as well.