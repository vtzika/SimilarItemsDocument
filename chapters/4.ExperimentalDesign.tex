\chapter{Experimental Design}
To answer our research questions, we conducted six different kind of experiments. In the following sections, we present different types of experiments, data, research questions and evaluation.



\section{Data and data gathering}

Our data collection is a dataset of 7,502,132 classifieds (8.8 GB of memory) which is the total number of active classifieds in the period of 1 month in the site of Marktplaats. Also, we remove classifieds that are suspended from the system as being duplicates. We choose 100 classifieds from the entire data collection to represent the previously visited classifieds, thus the user's information need or topic. The classifieds were uniformly formatted into a Standard Generalized Markup Language (SGML) structure with tags for each part of a classified, as can be seen in the listing 4.1.



{\small
\begin{code}[caption={SGML formated classified}]
<DOC>
	<DOCNO>244563422</DOCNO>
	 <TITLE>
		 koop huur rietgedekte villa landhuis praktijkruimte eerbeek
	</TITLE>
	<DESCRIPTION>
		 aangeboden exclusief rietgedekt modern landhuis eerbeek praktijkruimte eigen ingang koopprijs   998 000   kk   huurprijs   3 300   per maand
	</DESCRIPTION>
	<CATEGORY>
		 huizen en kamers huizen te koop  huizen koop
	</CATEGORY>
	<PRICE>
		€ 998.000
	</PRICE>
	<ATTRIBUTES>
	  Aantal kamers 5 kamers of meer Woonoppervlakte 150 m² of meer
	</ATTRIBUTES>
</DOC>
\end{code}
}

Indri search engine requires the SGML format. All classifieds have beginning, end markers and unique document number (DOCNO) field. Also, they consist of a title, a description, a price, a category and several attributes. 

With the use of Indri build Index application we build repositories from the document collection. The buildIndex application uses parameter files to create repositories of indexes of all the classifieds (see listing 4.2).

\begin{code}[caption={Build index parameter file}]
 <parameters>
	<index>/home/varvara/workspace/externalSources/indri/repositories2/mergedOutput24</index>
    <memory>1G</memory>
    <corpus>
      <path>/home/lemur/testdata/firstCorpus</path>
      <class>trectext</class>
    </corpus>
    <stemmer><name>krovetz</name></stemmer>
    <field>
      <name>p</name>
    </field>
  </parameters>
\end{code}


It is necessary for our data collection to be stored contiguously on disk for optimum retrieval performance \cite{TrevorStrohman}. We merged the 125 repositories in six repositories with the use of dumpIndex application to improve performance. Statistics for each repository of unstemmed data collection you can find in table 4.1 and stemmed in table 4.2.


\begin{table}[H]
\begin{center}
\footnotesize
\caption{Statistics of five unstemmed repositories (Rep1, Rep2, Rep3, Rep4 and Rep5). Total number of documents, unique terms and total terms for each repository is given.}
\begin{tabular}{lccccc}
\toprule
&Rep1 & Rep2 & Rep3 & Rep4 & Rep5 \\
\midrule
Documents & 1,440,000 & 1,403,211 & 1,440,000 & 1,440,000 & 1,440,000 \\
Unique terms & 1,126,145 & 1,138,034 & 1,135,226 & 1,057,184 & 1,066,359\\
Total Terms & 76,223,460 & 67,441,399 & 68,602,942 & 68,034,636 & 68,206,373\\
\bottomrule
\end{tabular}
\end{center}
\end{table}



\begin{table}[H]
\begin{center}
\footnotesize
\caption{Statistics of five stemmed repositories (Rep1, Rep2, Rep3, Rep4 and Rep5). Total number of documents, unique terms and total terms for each repository is given.}
\begin{tabular}{lccccc}
\toprule
&Rep1 & Rep2 & Rep3 & Rep4 & Rep5 \\
\midrule
Documents & 1,440,000 & 1,403,211 & 1,440,000 & 1,440,000 & 1,440,000 \\
Unique terms & 1,074,828 & 1,081,013 & 1,079,051 & 1,006,485 & 1,016,282 \\
Total Terms & 76,184,789 & 67,405,711 & 68,567,148 & 68,002,150 & 68,173,834 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}




\section{Retrieving classifieds list}

Having indexes of the document collection described above, the next step is to create an Indri-style query file like listing 4.3.

\begin{code}[caption={Query parameter file}]
<parameters>
 <index>/home/repositories/rep1</index>
  <query>
 	<text> koop huur rietgedekte villa </text>
	<number> 244 </number>
  </query>
  <baseline>tf.idf,k1:1.0,b:0.3</baseline>
  <count>30</count>
  <trecFormat>true</trecFormat>
</parameters>
\end{code}

From the 100 chosen classifieds, we create queries in Indri-style query files like in listing 4.3. Also, creating the query involves parsing the visited classified which we use as topic to find relevant classifieds. Parsing has to be the same as the preprocessing of the document collection. Otherwise the accuracy of results will be affected negatively and the results will not be optimal. For example, if we have a query term `books' then it will be difficult to find documents related to `book'.

Next, the query file runs against our repositories using IndriRunQuery and retrieves the relevant list of classifieds. The output of the IndriRunQuery is a TREC style file called qrels consist of relevant classifieds to a query. These files are then input to trec\_eval, to calculate the precision in five first results.


The different query models we use are the following:

\begin{list}{.}{}
\item{Title words}
\item{Title and description words (unstructured data)}
\item{Attributes and Category (structure data)}
\item{Structure data and unstructured data}
\item{LLR in Title words}
\item{LLR in Title and description words}
\item{LLR in Structure data and in unstructured data}
\end{list}


Different retrieval systems are used for experimentation purposes. As it is mentioned in section 3.4 , our three retrieval models are:
\begin{list}{.}{}
\item{Tf.Idf}
\item{Okapi BM25}
\item{LM}
\end{list}

\section{Our baseline}
The title of the classified is the summary of the classified. It's brief and consists of the most important information. For this reason, we use the title query model as baseline with okapi BM25 as retrieval strategy without stemming. We choose okapi which is proven that is a state of the art retrieval strategy and it performs better in our experiments in the comparison with the other two (LM, TfIdf). We are not using stemming because it harms our retrieval effectiveness as it is proven from our experiments.


\section{Experiments}

\subsection{Query modeling}
With this kind of experiment, we answer the first research question. For each of the three types of query models (classifieds' structure, discriminative terms, pseudo relevance feedback) we construct queries and submit them to our index of classifieds. We measure the performance of each system and we compare the results to evaluate the differences in performance between the models.


To create a good query model, the query has to contain the most important information from the classified. But how can we find words that contribute the most important information from a classified? In the case of classifieds we have several fields that important information can be found. Also, a lot of noise exists in the fields which harms the retrieval efficiency. Discriminative terms can be extracted by the classifieds fields. Furthermore, we can get feedback from the result lists and create new queries to improve our retrieval performance.

As a first step, we conducted an experiment to answer the first research question using query models with classified fields terms. We compare the precision of the resulted classified lists with our baseline's precision. 

Furthermore we make experiments using discriminative terms and pseudo relevance feedback that are presented in the following sections.

 \paragraph{Query models with discriminative terms}

As it is described in the methodology chapter, we will use the LLR to create queries with discriminative terms. We use LLR to extract discriminative terms from the title and the entire classified. Given the visited classified field or all fields as null corpora and the big dataset of 8.8 GB classifieds as normative corpora we will produce our queries.

 \paragraph{Query models with pseudo relevance feedback}

The method we follow in order to use pseudo relevance feedback is the same method used in normal retrieval. The system will use the results from the original query and extend it with the feedback. The system assumes that the top five ranked documents are relevant. It extracts the five most frequent terms in this top five ads, expands the query with this terms and finally retrieves results with the expanded query.



\subsection{Retrieval model}
We conduct this experiment to investigate which of the three retrieval models (Okapi BM25, TfIdf, LM) is performing best (second research question). All the query models available are part of the experiment. The three retrieval methods are already described in the methodology chapter.


\subsection{Late data fusion}

With the late data fusion experiments, we give answers to the third research question. We use late fusion techniques to fuse the individual models from the query modeling experiments and we produce new result lists. Then we compare their performance in contrast with individual models.


We experiment with eight fusion techniques as explained in the methodology chapter. The results were compared with the individual models to answer the relevant research question. Since we don't have any weight in the queries, we use the MAP as a weight of a sample of classified runs. So we choose 50 random visited classifieds and we use the MAP of the trec results run. Then, we use the MAP as the weight to the WCombSUM, WcombMNZ and WcombWW methods.

\subsection{Diversification}
To answer the fourth, fifth and sixth research question, we diversify first the query models from the first experiment with MMR method. Then, we diversify with our three diversity methods explained in method section. To evaluate the results, we compare them with the MMR method.

To evaluate these experiments we used the clicks logs evaluation method described below. The assessors evaluation is based one the assumption that we are searching relevant classifieds to the visited one. While in the case of diversification, we want more diversified results in case that we will increase the possibilities that will cover the information need of the user. Clicks indicate user's interest.

In the following paragraphs, we are presenting the experimental design of the three alternative diversification methods.

 \paragraph{MMRalt1}

The algorithm of this diversification method is provided in methodology chapter. It is based on the assumption that we don't want to show two similar results in a row. So, we are taking into account only the similarity between the previous selected classified and the unselected classifieds.

 \paragraph{MMRalt2}

The algorithm of this diversification method is provided in methodology chapter. The difference with the MMR implementation is that we are calculating the average similarity for each classified with all the selected classifieds.

 \paragraph{MMRaltAvgLst4}

To answer the sixth research question we implement the algorithm provided in methodology and we compared the results with MMR. This algorithm aim to create a diverisfified sets of four classiifieds instead of taking into account the diversity between all classifieds.

Since we were doing the experiments in an industrial environment, their decisions or experience affect our decision in some cases. They wanted to expose only five results as similar classifieds paginated. This creates the idea to use windows on comparison of similarity. So the basic idea is that we want to show 5 diversified results per page. We compare only the similarity of the not selected classifieds and the previous four displayed classifieds. The algorithm we use to implement is provided on methodology chapter.

\subsection{Data fusion of diversified result}
For the last experiment, using the same late fusion techniques as in the previous experiment, we merge the diversified result lists to answer the seventh research question. We measure the difference in performance comparing the new result lists with the fused results from the previous experiments. The evaluation of the results is based on the click log evaluation as well due to the reasons mentioned in the previous section.




\section{Evaluation}

To evaluate our results and answer the research questions, we need to know which documents are relevant and if they are retrieved by the specific system. We follow two ways of evaluation. One method is using editorial ground truth and the other one is using click logs.

\paragraph{Editorial evaluation}

In this kind of evaluation, three assessors are provided with 100 topics which in our case are the contents of the visited classifieds and they evaluate a ranked list of documents (results of each system). They judge documents as relevant or not.

The three components of a test collection are the document set, a set of information need statements called topics, and the relevance judgments that indicate which documents should be retrieved in response to a given topic. Of these three components the relevance judgments are the most expensive to produce \cite{BuckleyDimmick}. Within big document collections, judging all documents as relevant or not is  almost impossible due to the time it requires. Also based on \cite{KalervoKekalainen} the greater the ranked position of a relevant document (of any relevance level) the less valuable it is for the user, because the less likely it is that the user will examine the document. It would therefore be desirable from the assessor viewpoint highly relevant documents to be ranked higher in the retrieval results lists.

With the use of pooling we can judge only a subset of the results. In pooling, a set of documents to be judged for a topic (the `pool') is constructed by taking the union of the top $\lambda$ documents retrieved for the topic by a variety of different retrieval methods. Each document in the pool for a topic is judged for relevance, and documents not in the pool are assumed to be irrelevant to that topic.
Sakai and Mitamura \cite{SakaiMitamura} report the outcome of their experiment to investigate the effect of reducing both the topic set size and the pool depth and they prove that using 100 topics with depth-30 pools generally yields fewer errors than using 30 topics with depth-100 pools. For this reason, we choose to have 100 topics with depth -30 pools.


Due to the fact that different persons have different opinions about the relevance for the same document we decide to use four assessors with different background (two developers, one business analyst and a product owner). However, based on \cite{MaskariSandersonClough}, multiple assessors make errors which affect the assessment. Basically the reasons lie in the ambiguity of data or mistakes of annotators due to lack of motivation or knowledge. Also, non-expert assessors judging domain-specific queries make significant errors affecting system evaluation. When assessors are not closely managed or highly trained, mistakes must be common. For this reason we calculate the kappa coefficient (K) to check the reliability of judgments.
The kappa coefficient (K) measures pairwise agreement among a set of assessors making binary judgments, correcting for expected chance agreement:
\begin{equation}
K= P(A) - \frac{P(E)} {1-P(E)}
\end{equation}

where P(A) is the proportion of times that the assessors agree and P(E) is the proportion of times that we would expect them to agree by chance, calculated along the lines of the intuitive argument presented above \cite{Carletta}.

In the following table we present the annotation agreement between our assessors:

\begin{table}[H]
\begin{center}
\footnotesize
\caption{Inter annotator agreement between different assessors (developer a, developer b, business analyst and product owner). Proportion of times assessor agree on relevance (P(agree-rel)), proportion of times assessor agree on irrelevance (P(agree-irr)), dataset and k-measure. }
\begin{tabular}{llcccr}
\midrule
 Assessor a & Assessor b & DataSet & P(agree-rel) & P(agree-irr) & k-measure \\
 \midrule
	Developer a & Business analyst & 1,261 & 0.13 & 0.39 & 0.64 \\
	Developer a & Product owner & 2,148 & 0.25 & 0.24 & 0.57 \\
	Developer a & Developer b & 574 & 0.22 & 0.27 & 0.57 \\
	Business analyst & Product owner & 820 & 0.23 & 0.26 & 0.54 \\
	Developer b & Product owner & 574 & 0.15 & 0.37 & 0.52 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}


With the previous in mind, we take the following decisions:

\begin{list}{.}{}
\item{To evaluate if a document is relevant or not  we need the opinion of potential users of our system.}
\item{Assessors have to be Dutch speakers. Since my document collection is in Dutch, assessors must be native Dutch speakers as well. The understanding of the language has to be appropriate to understand entirely the contents of the document.}
\item{We use as assessors experts with different background in order to cover different kinds of users.}
\item{Assessors without any intentions for the project. We need unbiased answers on relevance.}
\item{Binary value for judgment: zero for irrelevant and one for relevant documents.}
\item{Assessor will see a list of relevant documents. However, this list will be unordered because we don't want to direct assessor's opinion about relevance.}
\end{list}




\paragraph{Clicks logs evaluation}

In this approach, we use click logs as an indication of relevance instead of the assessors judgment. Provided with click logs of four days, we create a relevant list of all the classifieds users visited in one session after the visited classified. More into details, we create a list with all the classifieds visited after the one of the 100 classifieds we choose to see as topic. Then, we count as relevant only the classifieds which more than five different users click on it. We did the same for all 100 topic classifieds.



\subsection{Measures}

TREC is an annual conference started in 1992 co-sponsored and masterminded by the US National Institute of Standards and Technology (NIST), but tracks are largely organized by the participant research groups. It has contributed to many advances in information retrieval techniques such as ranking algorithms, improving old ideas and encouraging new ideas and experimentation.

TREC\_EVAL is a tool designed for evaluation of various information retrieval systems. It handles collection of documents, queries, and relevance judgments. It takes two documents as input and it calculates various measures for retrieval system evaluation. The measure we are interested in is the precision at first five documents which is more important for similar classifieds.



Since, the experimental design of each experiment, the data we use and how we evaluate the experiments are explained, we can present the results of the experiments we conducted in the following chapter.