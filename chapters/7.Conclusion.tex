\chapter{Conclusion and Future work}

In the previous two chapters, we answered our research questions and we provided our contribution on each part of our approach. Ideas created for future work in each section are presented in this chapter that we summarize and conclude our thesis.

We presented multiple experiments to find the best performing system for retrieving similar classifieds. Similar classifieds is a list with classifieds recommended to the user based on his previous interest. We explored multiple ways to create this list by using the contents of the previous classified the user visited. We evaluated the performance of the results based on the precision which is affected by the relevant or irrelevant results that are retrieved. A company provided us with all the data we needed to implement the experiments. They also provided us assessors (persons that dudje each resulted similar classifieds list's item as rellevant or irrelevant) for the evaluation of the experiments.


We made experiments to find the best query model in a comparison with the title query model which is the one the company is already using and is considered our baseline. We investigated which is the best retrieval strategy. We fused all the systems to improve their precision. Furthermore, we made experiments for diversification of the results. Finally, we investigated if the fusion of the diversified results improves the precision. We evaluated our experiment results using two different indication of relevance, the editorial which was based on assessors judgment on relevance and click logs which was based on users history.


Results from the stemming experiment proved that the presicion of our baseline is decreased when we use stemming. Also, we present a big number of relevant results that are not retrieved when stemming is used. Thus, we decided to not use it in any other query model. In future work, more investigation is needed for the reasons that stemming decreased the precision.


Experimental evidence proves that the more information we add to the query model the better the system performs. Eventually, the query model based on the entire classified content performs the best. Also, during the editorial evaluation that we examined visited classifieds and lists with similar classifieds, we observed that attributes and category add a lot of information to the query. However, when it is used alone (without the title and description terms) is not enough to cover the information need. Furthermore, the experimental results shows us also that attributes and category alone in the query model is the worst performing system. In future work, the search query a user did to find the visited classified can be added in the query model as extra information to see if there is any improvement in precision. Also, change in the precision can be investigated when all the possible combinations of the fields on query modeling are used.



On an extra experiment we made, we proved that choosing discriminative terms instead of all terms of the classified is not performing as good in terms of precision. Also, there is no big difference on the amount of relevant results retrieved by the same systems using LLR or without. Thus, the difference in the precision of systems using LLR approach needs further investigation.

The results of the pseudo relevance feedback experiment, show us no improvement in the precision and we didn't use it in more query models.

On retrieval method experiments, okapi BM25 performs better than the other two retrieval models in both types of evaluations and in all query models. Therefore, it leaves us no doubt about which is the best retrieval method to improve our systems performance.

The experimental results for late data fusion experiments proved that none of the late fusion methods improves the precision of the best individual system. However, in an attempt to analyze it further and prove the reason that the results are not as expected, we conducted an extra experiment of combANZ keeping stable the systemic differences. With this experiment we improve the compANZ performance but it was not performing better than the best individual. More experiments for the rest of the fusion methods using the same approach can be investigated in the future.

Also, we compare four diversification approaches to find the best performing one. None of them shows any difference in precision either using editorial or click logs evaluation. However, diversification is an expensive procedure due to the fact that you have to check the text similarity between all classifieds in the result list. Thus, the first alternative diversification method (MMRalt1) that compares only the previous selected classified with all the non selected, can be proved better because it doesn't affect the precision and is faster. Same is for the third alternative method (MMRaltAvgLast4) which takes into account only the previous four selected classifieds. But the assumption that these are faster and the fact that this makes them better needs a further investigation and a proper benchmark to be proved. Also, we already made the assumption that maybe the results are already diversified and that's why we don't have any big difference on the precision of the top five first results. In future work the similarity of the classifieds to prove if this assumption is true or not can be investigated.

Finally, we experiment with the fusion of the diversified results and we compare them initially with the fused individual systems. The results indicate that fusion of diversified results improves the precision. We also compare the results with the best individual system from the query model experiment and it's proved that combMAX improves even the performance of the best individual system. In future plans, these refinements can be compared with the diversification of fused results from Liang et al approach \cite{LiangRenMaarten}.


In conclusion, we improved the precision of the similar classifieds baseline's query model. We also find the best retrieval strategy that results the greatest precision. We proved that fusion of results is not performing better than our individual best system but we achieved to improve the precision of one of the fusion methods. We proposed three alternative diversification methods but none of them had big improvement in a comparison with the MMR \cite{CarbonellGoldstein}. Finally, we fused the diversified results and we achieved to have the greatest precision of all the experiments. 
